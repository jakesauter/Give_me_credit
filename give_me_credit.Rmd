---
title: "Give Me Credit -- Notebook"
output: 
  github_document:
    toc: true
    df_print: kable
---

# Phase 1: Data Exploration / Correction 

We will need **dplyr** in order to make sense of our 
data as well as make possible modifications. **ggplot2** will 
be a helpful visualization tool during this journey. **purrr**
will assist us in it useful maping function for iteration.

```{r setup, include=FALSE}
# knitr::opts_chunk$set(fig.height = 3)
# knitr::opts_chunk$set(fig.width = 5)
knitr::opts_chunk$set(cache = FALSE)
```

```{r includes, message=FALSE, warning=FALSE}
library(dplyr)
library(purrr)
library(ggplot2)
library(cowplot)
```

```{r}
source("R/utils.R")
```

Now lets load in the data and begin exploring! The first
thing we will do is take a look at what variables we have 
to work with and their types. 

```{r}
df <- load_all_training_data()

names(df) %>% 
  map(~tibble(Var = .x, Type = typeof(df[[.x]][1]))) %>% 
  bind_rows
```

I always think it is a good idea to see a few samples
of they data you are working with as well, so here are 
the first 5 rows.

```{r}
df %>% head(n = 5L)
```

Lets take a look at how balanced the labels that we
are trying to predict are in the dataset we are given. 

```{r}
df %>% 
  summarise(mean(SeriousDlqin2yrs))
```

We see that the mean of this set of binary labels is 
about .066, showing that the dataset is mostly filled 
with people who did not default on their loans. 


## Age vs Serious Deliquency

Plot the distributions of people who defaulted vs age,
in general we should see that younger people default more
as credit agencies must give mortgage to those who are
55 and older

```{r}
ggplot(df) +
  geom_histogram(aes(x = age), bins = 20) + 
  facet_wrap(~SeriousDlqin2yrs)
```


## Debt Ratio 

Now lets get a 5 number summary of the debt ratio
```{r}
df %>% 
  select(DebtRatio) %>% 
  summary
```

As a developing statistician, it is a good thing to note that 
this is the same analysis as rounded quanitles by quarters (quartiles)

```{r}
df$DebtRatio %>% quantile(probs = seq(0, 1, .25)) %>% round(2)
```

Its concerning that we see that someone owes 300,000 times
more than they own, is this person a single outlier?

```{r}
df$DebtRatio %>% quantile(.975)
```

From this statistic we can see that 2.5% of the dataset owes more 
than 3,500 times what they own. We need to investigate further
to see if these are outliers or not 

```{r}
df %>% 
  filter(DebtRatio > 3489.025) %>% 
  select(DebtRatio, MonthlyIncome) %>% 
  summary
```


## Validating Monthly Income Outliers

We see that these outliers either have a monthly income
of 0, 1 or NA. This seems like it could be a data-entry error, lets see if 
there is any correlation between Serious Deliquency rate 
and the monthly income for these examples

```{r}
corr <- df %>% 
  filter(DebtRatio > 3489.025 & !is.na(MonthlyIncome)) %>% 
  select(SeriousDlqin2yrs, MonthlyIncome) %>% 
  mutate(corr = SeriousDlqin2yrs == MonthlyIncome) %>% 
  select(corr) %>% 
  unlist %>% unname
```

```{r}
corr %>% 
  table
```

```{r}
corr %>% mean
```

We see that out of the 184 samples that have a valid
monthly income, 164 of them (88.6%) have the same value for 
Seriously Deliquent as for monthly income, so this
must be a data entry error 

We see this corrolation, but are these people defaulting on
their credit more than usual? 

```{r}
df %>% 
  filter(DebtRatio > 3489.025 & !is.na(MonthlyIncome)) %>% 
  select(SeriousDlqin2yrs) %>% 
  summarise(mean = mean(SeriousDlqin2yrs)) 
```

I couldn't quite understand why/how this subset of 
candiates owe 3500 times more than they are generating
and do not have a higher than usual default rate. A
hypothesis I have include that they have possilby 
filed for bankruptcy and are making the payments that
they can afford

At any rate it does seem much like an outlier case 
and can be filtered out before modelling 
as these are stand out cases that the model
should probably not be predicting anyway 

```{r}
df <- df %>% 
  filter(!is.na(MonthlyIncome) && DebtRatio < 3489.025)
```

## Number of 90 Days Late

Now lets take a look at the number of 90 days late variable

```{r}
df %>%
  select(NumberOfTimes90DaysLate) %>% 
  unlist %>% 
  table %>%
  as.data.frame %>% 
  set_names(c('Value', 'Frequency'))
```

It is interesting that no one is between 17 and 96 times late. Lets take 
a lot at these records

```{r}
df %>%
  filter(NumberOfTimes90DaysLate > 95) %>% 
  select(SeriousDlqin2yrs, 
         NumberOfTime30.59DaysPastDueNotWorse, 
         NumberOfTime60.89DaysPastDueNotWorse, 
         NumberOfTimes90DaysLate) %>% 
  summary
```

This sample of subjects has the same exact number of times
late for all three recorded categories, though this might
not be an entry error as they do have a very high default
rate at around 55%, compared to the popululation seen
above at 6%


### Winsorization of Late Variables

This might be a good time to implement Winsorization
https://en.wikipedia.org/wiki/Winsorizing -- The act of 
transforming statistics by limiting extreme values to 
reduce the effect of outliers, having the same effect as 
amplitude clipping in signal processing.

To implmenent winsorization here we will let these 
number of defauling times be 20s instead of 96 and 98 

```{r}
mutate_vars <- c("NumberOfTime30.59DaysPastDueNotWorse",
                 "NumberOfTime60.89DaysPastDueNotWorse", 
                 "NumberOfTimes90DaysLate") 


for (var in mutate_vars) {
  df[df[[var]] > 20, var] <- 20 
}
```

```{r}
df %>%
  select(SeriousDlqin2yrs, 
         NumberOfTime30.59DaysPastDueNotWorse, 
         NumberOfTime60.89DaysPastDueNotWorse, 
         NumberOfTimes90DaysLate) %>% 
  summary
```

## Revolving Utilization of Unsecured Lines

The last variable that we are going to take a look at is 
revolving utilization of unsecured lines (RUUL)

RUUL represents the ratio of money owed to credit limit, 
so it should really not be above 1

More on Revovling Utilization here
https://finance.yahoo.com/news/credit-101-revolving-utilization-060108050.html

```{r}
df %>% 
  select(RevolvingUtilizationOfUnsecuredLines) %>% 
  summary
```

Possibly more interesting than an individual having a debt ratio
greater than 2500 is an individual having a RUUL of 50,708. 
Something is most likely wrong with this entry. 
Lets first examine different levels of credit and how much they are defaulting 

```{r}
x <- df %>% 
  select(RevolvingUtilizationOfUnsecuredLines, 
         SeriousDlqin2yrs) %>% 
  filter(RevolvingUtilizationOfUnsecuredLines >= .9, 
         RevolvingUtilizationOfUnsecuredLines < 4)
```

```{r}
x %>% 
  count 
```

```{r}
x %>% summary
```

These entries have a default rate of ~ 22.5% -- almost 1 in 4.

Lets consider 4 to 10 as well.

```{r}
x <- df %>% 
  select(RevolvingUtilizationOfUnsecuredLines, 
         SeriousDlqin2yrs) %>% 
  filter(RevolvingUtilizationOfUnsecuredLines > 4, 
         RevolvingUtilizationOfUnsecuredLines <= 10)
```

```{r}
x %>% count
```

```{r}
x %>% 
  summary %>% 
  as.list %>% 
  as.data.frame %>% 
  t()
```

There are only 23 records in this region, but they are defaulting
at a very high rate, with this in mind lets take a look at records
with an even higher RUUL

```{r}
x <- df %>% 
  select(RevolvingUtilizationOfUnsecuredLines, 
         SeriousDlqin2yrs) %>% 
  filter(RevolvingUtilizationOfUnsecuredLines > 10) 
```

```{r}
x %>% count
```

```{r}
x %>% summary
```

But how can people have more 
debt than their credit limit? It could be from individuals
maxxing out a credit card and having it closed -- or if they
do not make an income and have a debt -- their account income
might be set to $0.01 instead of 0 as to not lead to a NaN value. 

After investigating further we see that these 241 people are not defaulting
at any higher of a rate than the standard population. Again this is a situation
that we might want to filter out before classification through an svm
as this might throw off our classifier. 

```{r}
df <- df %>% 
  filter(RevolvingUtilizationOfUnsecuredLines < 10)
```

Finally lets take a look at using regression to fill in our 
missing values for Monthly Income as this seems to be an important
feature that we would want to have for every sample

```{r}
clean_df <- df %>% 
  drop_na

model <- lm(MonthlyIncome~., clean_df)

model$residuals %>% range
```

```{r}
model$residuals %>% 
  as.data.frame() %>% 
  ggplot + 
  geom_histogram(aes(x = .))
```

```{r}
model$coefficients
```

The **summary** function is overridden for model types so we can see the r squared there.

```{r}
model %>% summary
```

Our adjusted R squared value (penalizes for model size) is .022 -- only
slightly better than guessing the mean

We see that we have a very significant F-statistic, though this does not
automatically mean that our model is amazing -- Just because we have
statistical significance does not mean that we have practical significance


In short, this is bad so we will replace missing values 
with median value (robust to outliers)

```{r}
median_monthly_income <- df$MonthlyIncome[!is.na(df$MonthlyIncome)] %>% median
df$MonthlyIncome[is.na(df$MonthlyIncome)] <- median_monthly_income
x <- df$MonthlyIncome %>% .[. < quantile(., .95)]
```

```{r}
x %>% 
  as.data.frame %>% 
  ggplot +
  geom_histogram(aes(x = .), bins = 100)
```

30k entries did not have a monthly income so thats why so many
people have the median value that we calculated 

# Phase 2: Modelling 

Now that we have some understanding of our data and have filtered out cases
that are easily discernable or outliers that will negatively affect our model, 
we are ready to begin exploring the most effective class of model for 
our dataset 

Due to the amount of outliers in our data, it would be best to start with 
a classifier robust to these, such as the **Random Forest** model. A random 
forest is simplistic model, though has proven itself to be
very effective in many fields.

## Random Forest 

We will use the **randomForest** package in R for this model. 

```{r}
library(randomForest)
```

I have wrote a few functions to help with loading the data 
with proper filtering, as well as to help with model assessment. 

```{r}
load_model_data()

train$SeriousDlqin2yrs %<>% as.factor()
xtest <- test %>% select(-SeriousDlqin2yrs)
ytest <- test$SeriousDlqin2yrs %>% as.factor
```

Note that we must translate the target variable to a factor to 
use a classification tree as opposed to a regression tree
in the randomForest package. Once our model is trained we can 
save it into rds format so that it is reloadable.

```{r }
if (!exists('rf_model')) {
  rf_model <- readRDS(file = "rf_model_2500.rds")
}

if (!exists('raw_preds') && !file.exists('predictions.rds')) {
  raw_preds <- predict(rf_model, test %>% select(-SeriousDlqin2yrs), type = 'prob')
  saveRDS(object = raw_preds, file = 'predictions.rds')
} else if (!exists('raw_preds')) {
  raw_preds <- readRDS('predictions.rds')
}

preds <- raw_preds[,2] %>% unname

confusion_matrix(test$SeriousDlqin2yrs, preds, clf_thresh = .5)
```


```{r}
roc <- simple_roc(test$SeriousDlqin2yrs, preds)
```


```{r}
clf_thresh <- roc %>% select_threshold(FPR = .25)

confusion_matrix(test, preds, clf_thresh = clf_thresh)
```

## SVM 

svm model here

## Naieve Bayes

model here

## neural net 

neural net here 






